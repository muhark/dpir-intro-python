{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Tutorial Week 8\n",
    "\n",
    "In this tutorial, we look at ways to combine `spaCy`, `regex`, `pandas`, `matplotlib` and `seaborn` to analyse the text column of the BES data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "bes_df = pd.read_feather(\"../Week2/data/bes_data_subset_week2.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now import spacy and our language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\") # if this doesn't work for you\n",
    "# open ipython terminal\n",
    "# >>> import spacy\n",
    "# >>> nlp = spacy.load(\"en_core_web_sm\")\n",
    "# >>> nlp._path\n",
    "# PosixPath('<COPY THIS>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what spacy can do, let's use it on one of the short responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(bes_df.loc[1216, 'a01'])\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.pos_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_spacy_data(doc):\n",
    "    \"View various aspects of the language model.\"\n",
    "    data = []\n",
    "    for token in doc:\n",
    "        data.append([token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "                     token.shape_, token.is_alpha, token.is_stop])\n",
    "    columns = ['text', 'lemma_', 'pos_', 'tag_',\n",
    "               'dep_', 'shape_', 'is_alpha', 'is_stop']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_spacy_data(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a bit of pre-processing before we apply the language model to the rest of the answers.\n",
    "\n",
    "- NA removal\n",
    "- lowercase everything\n",
    "- remove consecutive spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bes_df['a01'].value_counts().head(20) # Looks like '-1' is a na value\n",
    "                                      # Let's drop all rows that are na here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bes_df.loc[bes_df['a01']!='-1', :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['a01'].apply(lambda x: type(x)).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'a01'] = df['a01'].str.lower().str.replace(re.compile(r\"\\s{2,}\"), \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nlp'] = df['a01'].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(doc):\n",
    "    nouns = [token.lemma_ for token in doc if\n",
    "             token.pos_ in ('PROPN', 'NOUN') and\n",
    "             token.is_stop==False]\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nouns'] = df['nlp'].apply(get_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_frequencies = pd.Series(Counter(df['nouns'].sum())).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,1, figsize=(15, 9))\n",
    "\n",
    "ax.set_title(\"Top 50 Most Common Nouns in Item a01\")\n",
    "sns.barplot(noun_frequencies.head(50).index, noun_frequencies.head(50), ax=ax)\n",
    "ax.xaxis.set_ticklabels(ax.xaxis.get_ticklabels(), rotation=-90)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Cosine Distance Clustering\n",
    "\n",
    "This model will take more time than we have in class to evaluate, but here's a bit of code that can show you how to conduct cosine clustering on your document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vector'] = df['nlp'].apply(lambda x: x.vector)\n",
    "df = df.loc[df['vector'].apply(lambda x: np.any(x)), :] # Dropping zero vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = np.vstack(df['vector'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vector'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vector'].apply(lambda x: type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_cluster = AgglomerativeClustering(n_clusters=30, affinity=\"cosine\", linkage=\"single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_cluster.fit(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cos_labs'] = cosine_cluster.labels_\n",
    "df.loc[:, 'cos_labs'] = df['cos_labs'].astype(pd.CategoricalDtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cos_labs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in range(10):\n",
    "    n_samp = min(sum(df['cos_labs']==label), 3)\n",
    "    sample = df.loc[df['cos_labs']==label, 'a01'].sample(n_samp)\n",
    "    print(\"###### CLUSTER \"+str(label)+\" ######\")\n",
    "    for item in sample.iteritems():\n",
    "        print(item[1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously there's a lot of work to be done here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
