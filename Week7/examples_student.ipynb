{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Tutorial Week 7: Web Scraping\n",
    "\n",
    "This week focuses on three skills that used together, allow you to perform web scraping.\n",
    "\n",
    "We work on them in the following order:\n",
    "\n",
    "1. Regular Expressions `re`\n",
    "2. Making `html` requests\n",
    "3. Parsing `html` with `beautifulsoup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions\n",
    "\n",
    "Regular expressions are a tool for matching sequences of characters (i.e. strings).\n",
    "\n",
    "The built-in library `re` contains most functionality we need for this lecture, but for broader unicode support you may want to use `regex` (which needs to be installed).\n",
    "\n",
    "To begin, let's start with the sentence _How now brown cow?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How now brown cow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sentence = \"How now brown cow?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a regular expression, you can use a regular Python string, but often it is preferable to compile a raw string.\n",
    "\n",
    "We do this by using the `re.compile` function on a string prefaced by `r`.\n",
    "\n",
    "Let's first create a pattern that matches the word \"cow\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"cow\")\n",
    "pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several functions that can be used with this pattern. The ones we are interested in are:\n",
    "\n",
    "- `re.search`\n",
    "    - Related: `re.findall`\n",
    "- `re.split`\n",
    "- `re.sub`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember: the pattern is \"cow\"\n",
    "\n",
    "match = re.search(pattern, sentence)\n",
    "print(match[0])\n",
    "\n",
    "print(re.findall(pattern, sentence))\n",
    "print(re.split(pattern, sentence))\n",
    "print(re.sub(pattern, \"fox\", sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a literal search pattern does not really demonstrate the power of regular expressions.\n",
    "\n",
    "The pattern `\\w+ow\\b` will match words that end with \"ow\".\n",
    "\n",
    "- `\\w` indicates the set of all letters.\n",
    "    - `+` indicates 1 or more occurrences of the previous RE.\n",
    "- `o`, `w` are the exact letters `o` and `w`.\n",
    "- `\\b` matches the null string at the start or end of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence)\n",
    "pattern = re.compile(r\"\\w+ow\\b\")\n",
    "\n",
    "print(re.search(pattern, sentence)) # Only finds the first instance\n",
    "print(re.findall(pattern, sentence)) # Returns all matches\n",
    "print(re.sub(pattern, \"fox\", sentence)) # Substitutes all matches with \"fox\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we only want to match \"How\" and \"cow\": `[Hc]ow\\b`\n",
    "\n",
    "- `[Hc]` matches the letters `H` or `c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence)\n",
    "pattern = re.compile(r\"[Hc]ow\")\n",
    "\n",
    "print(re.search(pattern, sentence)) # Only finds the first instance\n",
    "print(re.findall(pattern, sentence)) # Returns all matches\n",
    "print(re.sub(pattern, \"fox\", sentence)) # Substitutes all matches with \"fox\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match all lowercase words: `\\b[a-z]+\\b`\n",
    "\n",
    "- `\\b` matches the null string at the beginning of the word.\n",
    "- `[a-z]` defines the set of all lowercase latin letters.\n",
    "    - `+` indicates one or more of the previous RE\n",
    "- `\\b` matches the null string at the end of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence)\n",
    "pattern = re.compile(r\"\\b[a-z]+\\b\")\n",
    "\n",
    "print(re.search(pattern, sentence)) # Only finds the first instance\n",
    "print(re.findall(pattern, sentence)) # Returns all matches\n",
    "print(re.sub(pattern, \"fox\", sentence)) # Substitutes all matches with \"fox\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Irish Rover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = \"\"\"\n",
    "On the Fourth of July, 1806\n",
    "We set sail from the sweet Cove of Cork\n",
    "We were sailing away with a cargo of bricks\n",
    "For the Grand City Hall in New York\n",
    "'Twas a wonderful craft, she was rigged fore and aft\n",
    "And oh, how the wild wind drove her\n",
    "She stood several blasts, she had twenty seven masts\n",
    "And they called her The Irish Rover\n",
    "We had one million bags of the best Sligo rags\n",
    "We had two million barrels of stone\n",
    "We had three million sides of old blind horses hides\n",
    "We had four million barrels of bones\n",
    "We had five million hogs and six million dogs\n",
    "Seven million barrels of porter\n",
    "We had eight million bails of old nanny goats' tails\n",
    "In the hold of the Irish Rover\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract all of the words that begin with a capital letter and are 3 letters or longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile() # Fill in the blank\n",
    "\n",
    "re.findall(r, lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's substitute all the \"x million\"s an actual number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight']\n",
    "for i, number in enumerate(numbers, 1): # Start from 1\n",
    "    lyrics = re.sub(\n",
    "        re.compile(number+\" million\", re.IGNORECASE), # re.IGNORECASE to include Seven\n",
    "        str(i)+\",000,000\",\n",
    "        lyrics)\n",
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sub-Groups and Greedy Matching\n",
    "\n",
    "- Use `()` to create capture groups within your regex.\n",
    "- By default, `+*?` will match as many instances as possible.\n",
    "- Add an additional `?` after a pattern to match as few instances as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "s = '<html><head><title>Regex</title>'\n",
    "# Let's get the title\n",
    "\n",
    "r = re.compile(r\"<title>([A-z ]+)</title>\")\n",
    "re.findall(r, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Greedy matching - how do we get just one tag?\n",
    "r = re.compile(r'<.*>')\n",
    "\n",
    "re.findall(r, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Non-greedy solution\n",
    "r = re.compile(r'<.*?>')\n",
    "\n",
    "re.findall(r, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Challenge: Let's extract all the \"x million ___\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile() # Fill in the blank\n",
    "\n",
    "re.findall(r, lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requests\n",
    "\n",
    "We use the `requests` library to make `html` requests and retrieve webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `requests`, we first initiate a `Session` object, and then use the `.get()` method of the `Session`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://books.toscrape.com\"\n",
    "\n",
    "session = requests.Session()\n",
    "page = session.get(url)\n",
    "print(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good practice to use `try`/`except` when making the original request.\n",
    "\n",
    "`try`/`except` runs the code under `try` until an _exception_, or error, occurs.\n",
    "\n",
    "If we try to use the url `books.toscrape.com`, with no schema, then we will get a `requests.exceptions.MissingSchema` error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"books.toscrape.com\"\n",
    "\n",
    "try:\n",
    "    session = requests.Session()\n",
    "    page = session.get(url)\n",
    "except requests.exceptions.MissingSchema as e:\n",
    "    url = \"http://\"+url\n",
    "    print(\"Retrying with \"+url)\n",
    "    session = requests.Session()\n",
    "    page = session.get(url)\n",
    "    print(\"Request returned Status Code \"+str(page.status_code))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "\n",
    "BeautifulSoup contains tools for navigating `html` code.\n",
    "\n",
    "Although not strictly necessary, given that `html` does not need to be \"complete\" (i.e. missing tags are permissible), libraries such as this are much easier to use than trying to directly parse raw `html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can just print out the entire document\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the header\n",
    "\n",
    "print(soup.head.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get all links in the page\n",
    "\n",
    "links = soup.find_all('a', href=True)\n",
    "\n",
    "for link in links:\n",
    "    print(link.text, link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That output is annoying, so let's use some regex to remove whitespace\n",
    "\n",
    "for link in links:\n",
    "    print(re.sub(re.compile(r\"\\s\"), \"\", link.text), link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It'd be easier to read in a table, so let's use pandas!\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "links_df = pd.DataFrame(\n",
    "    data={\n",
    "        \"Label\": [re.sub(re.compile(r\"\\s\"), \"\", link.text) for link in links],\n",
    "        \"Link\": [link['href'] for link in links]\n",
    "    }\n",
    ")\n",
    "links_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The latter links look a bit strange; there is missing text.\n",
    "\n",
    "links_df[links_df['Label'].apply(lambda x: len(x)==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = pd.DataFrame(\n",
    "    data={\n",
    "        \"Label\": [re.sub(re.compile(r\"\\s\"), \"\", link.text) for link in links if re.search(re.compile(r\"\\S\"), link.text)],\n",
    "        \"Link\": [link['href'] for link in links if re.search(re.compile(r\"\\S\"), link.text)]\n",
    "    }\n",
    ")\n",
    "links_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Inspector with Web Scraping\n",
    "\n",
    "When it's just one webpage we want to scrape repeatedly, or a series of similarly-formatted webpages, we can use the Inspector to see if there are any patterns we may be able to use.\n",
    "\n",
    "Let's try and get information on all of the books listed on the first page of the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, the information all seems to be contained in `html` tags called `<article class=\"product_pod\">`.\n",
    "\n",
    "We can use this to isolate all of the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_pods = soup.find_all('article', class_=\"product_pod\")\n",
    "len(product_pods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at just one of the elements\n",
    "\n",
    "print(product_pods[0].prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_info(product_pod):\n",
    "    # Title can be accessed from img alt\n",
    "    image_elem = product_pod.div.a.img\n",
    "    title = image_elem['alt']\n",
    "    # Rating can be accessed from class (css) on star-rating\n",
    "    rating_elem = product_pod.find('p', class_=re.compile(r\"star-rating .*\"))\n",
    "    rating = rating_elem['class'][1]+\"/Five\" # Second class attribute\n",
    "    price_elem = product_pod.find('div', class_=\"product_price\")\n",
    "    price = re.search(re.compile(\"[0-9\\.]+\"), price_elem.text)[0]\n",
    "    link = product_pod.find('a', href=True)['href']\n",
    "    return title, rating, price, link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_info = []\n",
    "for pod in product_pods:\n",
    "    product_info.append(get_product_info(pod))\n",
    "\n",
    "product_info = pd.DataFrame(product_info, columns=[\"Title\", \"Rating\", \"Price\", 'Link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_info.loc[:, 'Link'] = product_info['Link'].apply(lambda x: url+\"/\"+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
